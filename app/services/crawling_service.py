import requests
from bs4 import BeautifulSoup
import urllib3
import re
import time

# SSL ê²½ê³  ë¬´ì‹œ
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


def get_detail_info(pbancSn):
    """
    ìƒì„¸ í˜ì´ì§€ì—ì„œ ìš”ì²­ëœ í•­ëª©ë“¤ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
    """
    url = f"https://www.k-startup.go.kr/web/contents/bizpbanc-ongoing.do?pbancClssCd=PBC010&schM=view&pbancSn={pbancSn}"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    # ê¸°ë³¸ê°’ ì„¤ì •
    info = {
        "tracks": [],
        "region": "",
        "target": "",
        "age": "",
        "period": "",
        "experience": "",
        "organizer": "",
        "required_docs": "",
        "deadline": ""
    }

    try:
        response = requests.get(url, headers=headers, verify=False)
        soup = BeautifulSoup(response.text, 'html.parser')

        # 1) ìƒë‹¨ ìš”ì•½ ë°•ìŠ¤ íŒŒì‹±
        items = soup.select(".information_box-wrap ul li")
        for item in items:
            key_tag = item.select_one(".tit")
            val_tag = item.select_one(".txt")
            if key_tag and val_tag:
                key = key_tag.get_text(strip=True)
                val = val_tag.get_text(strip=True)

                if "ì§€ì›ë¶„ì•¼" in key:
                    info["tracks"] = [x.strip() for x in val.split(",")]  # ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                elif "ì§€ì—­" in key:
                    info["region"] = val
                elif "ëŒ€ìƒ" in key and "ì—°ë ¹" not in key:
                    info["target"] = val
                elif "ëŒ€ìƒì—°ë ¹" in key:
                    info["age"] = val
                elif "ì ‘ìˆ˜ê¸°ê°„" in key:
                    info["period"] = val
                    # ë§ˆê°ì¼ ì¶”ì¶œ ë¡œì§ (ê¸°ê°„ ë¬¸ìì—´ì—ì„œ ë’¤ìª½ ë‚ ì§œ íŒŒì‹±)
                    dates = re.findall(r"\d{4}-\d{2}-\d{2}", val)
                    if len(dates) >= 2:
                        info["deadline"] = dates[1]
                    elif len(dates) == 1:
                        info["deadline"] = dates[0]
                elif "ì°½ì—…ì—…ë ¥" in key:
                    info["experience"] = val
                elif "ì£¼ê´€ê¸°ê´€" in key:
                    info["organizer"] = val

        # 2) ì œì¶œì„œë¥˜ íŒŒì‹±
        info_lists = soup.select(".information_list")
        for section in info_lists:
            title_p = section.select_one(".title")
            if title_p and "ì œì¶œì„œë¥˜" in title_p.get_text():
                doc_text = section.get_text(strip=True).replace("ì œì¶œì„œë¥˜", "").strip()
                info["required_docs"] = doc_text

        return info

    except Exception as e:
        print(f"âŒ ìƒì„¸ í˜ì´ì§€ íŒŒì‹± ì‹¤íŒ¨ ({pbancSn}): {e}")
        return info


def crawl_k_startup(page_limit=1):
    """
    ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒí•˜ë©° ê° ê³µê³ ì˜ ëª¨ë“  ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    base_url = "https://www.k-startup.go.kr/web/contents/bizpbanc-ongoing.do"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    results = []

    for page in range(1, page_limit + 1):
        print(f"ğŸ“¡ {page}í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘...")
        params = {"schM": "list", "pageIndex": page, "schStr": ""}

        try:
            response = requests.get(base_url, headers=headers, params=params, verify=False)
            soup = BeautifulSoup(response.text, 'html.parser')
            list_items = soup.select("#bizPbancList > ul > li")

            if not list_items:
                break

            for item in list_items:
                title_tag = item.select_one(".tit_wrap .tit")
                if not title_tag: continue

                title = title_tag.get_text(strip=True)

                # ê³µê³  ID ë° URL ì¶”ì¶œ
                link_tag = item.select_one("a")
                pbanc_sn = None
                full_url = ""

                if link_tag:
                    href = link_tag.get('href')
                    match = re.search(r"go_view\((\d+)\)", href)
                    if match:
                        pbanc_sn = match.group(1)
                        full_url = f"https://www.k-startup.go.kr/web/contents/bizpbanc-ongoing.do?pbancClssCd=PBC010&schM=view&pbancSn={pbanc_sn}"

                if pbanc_sn:
                    # ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                    detail = get_detail_info(pbanc_sn)

                    # ìˆ˜ì§‘ëœ ë°ì´í„° í†µí•©
                    comp_data = {
                        "external_id": pbanc_sn,
                        "name": title,
                        "url": full_url,
                        # ìƒì„¸ ë°ì´í„° ë³‘í•©
                        "deadline": detail["deadline"],
                        "tracks": detail["tracks"],
                        "region": detail["region"],
                        "target": detail["target"],
                        "age": detail["age"],
                        "period": detail["period"],
                        "experience": detail["experience"],
                        "organizer": detail["organizer"],
                        "required_docs": detail["required_docs"],
                    }
                    results.append(comp_data)
                    time.sleep(0.5)  # ì„œë²„ ë¶€í•˜ ë°©ì§€

        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì—ëŸ¬: {e}")

    return results